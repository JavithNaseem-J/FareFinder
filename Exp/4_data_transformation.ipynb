{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\Flight-Fare-Price-Prediction\\\\Exp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DS&ML\\\\Flight-Fare-Price-Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    target_column: str\n",
    "    preprocessor_path: Path\n",
    "    label_encoder: Path\n",
    "    categorical_columns: list\n",
    "    numerical_columns: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlproject.constants import *\n",
    "from mlproject.utils.common import read_yaml,create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    class ConfigurationManager:\n",
    "        def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "            schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "            self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root])\n",
    "\n",
    "        def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "            config = self.config.data_transformation\n",
    "            schema = self.schema\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            data_transformation_config = DataTransformationConfig(\n",
    "                root_dir=config.root_dir,\n",
    "                data_path=config.data_path,\n",
    "                target_column=config.target_column,\n",
    "                preprocessor_path=config.preprocessor_path,\n",
    "                label_encoder=config.label_encoder,\n",
    "                categorical_columns=schema.categorical_columns,\n",
    "                numerical_columns=schema.numeric_columns\n",
    "            )\n",
    "            \n",
    "            return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from mlproject import logger\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.target_column = config.target_column\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "        self.categorical_columns = config.categorical_columns\n",
    "        self.numerical_columns = config.numerical_columns\n",
    "        \n",
    "\n",
    "    def preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            data = data.copy()\n",
    "            \n",
    "                        \n",
    "            if self.target_column not in data.columns:\n",
    "                if 'Total Fare (BDT)' in data.columns:\n",
    "                    logger.info(f\"Renaming 'Total Fare (BDT)' to '{self.target_column}'\")\n",
    "                    data.rename(columns={'Total Fare (BDT)': self.target_column}, inplace=True)\n",
    "                else:\n",
    "                    raise ValueError(f\"Target column '{self.target_column}' not found in data\")\n",
    "            \n",
    "            for column in self.categorical_columns:\n",
    "                if column in data.columns:\n",
    "                    le = LabelEncoder()\n",
    "                    data[column] = le.fit_transform(data[column].astype(str))\n",
    "                    self.label_encoders[column] = le\n",
    "            \n",
    "            os.makedirs(os.path.dirname(self.config.label_encoder), exist_ok=True)\n",
    "            joblib.dump(self.label_encoders, self.config.label_encoder)\n",
    "            logger.info(f\"Saved label encoders to {self.config.label_encoder}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in preprocess_data: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def train_test_splitting(self):\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {self.config.data_path}\")\n",
    "            data = pd.read_csv(self.config.data_path)\n",
    "            \n",
    "            data = self.preprocess_data(data)\n",
    "            \n",
    "            train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "            \n",
    "            train_path = Path(self.config.root_dir) / \"train.csv\"\n",
    "            test_path = Path(self.config.root_dir) / \"test.csv\"\n",
    "            train.to_csv(train_path, index=False)\n",
    "            test.to_csv(test_path, index=False)\n",
    "\n",
    "            logger.info(\"Split data into training and test sets\")\n",
    "            logger.info(f\"Training data shape: {train.shape}\")\n",
    "            logger.info(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "            return train, test\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in train_test_splitting: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def preprocess_features(self, train, test):\n",
    "        try:\n",
    "            # Identify numerical and categorical columns\n",
    "            numerical_columns = train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "            categorical_columns = train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "            if self.target_column in numerical_columns:\n",
    "                numerical_columns = numerical_columns.drop(self.target_column)\n",
    "\n",
    "            logger.info(f\"Numerical columns: {list(numerical_columns)}\")\n",
    "            logger.info(f\"Categorical columns: {list(categorical_columns)}\")\n",
    "\n",
    "            # Preprocessing pipelines\n",
    "            num_pipeline = Pipeline(steps=[\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ])\n",
    "            \n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", num_pipeline, numerical_columns)\n",
    "                ],\n",
    "                remainder=\"passthrough\"\n",
    "            )\n",
    "\n",
    "            # Separate features and target\n",
    "            train_x = train.drop(columns=[self.target_column])\n",
    "            test_x = test.drop(columns=[self.target_column])\n",
    "            train_y = train[self.target_column]\n",
    "            test_y = test[self.target_column]\n",
    "\n",
    "            train_processed = preprocessor.fit_transform(train_x)\n",
    "            test_processed = preprocessor.transform(test_x)\n",
    "\n",
    "            # Ensure target is 2D array\n",
    "            train_y = train_y.values.reshape(-1, 1)\n",
    "            test_y = test_y.values.reshape(-1, 1)\n",
    "\n",
    "            train_combined = np.hstack((train_processed, train_y))\n",
    "            test_combined = np.hstack((test_processed, test_y))\n",
    "\n",
    "            # Save preprocessor using Path\n",
    "            joblib.dump(preprocessor, self.config.preprocessor_path)\n",
    "            logger.info(f\"Preprocessor saved at {self.config.preprocessor_path}\")\n",
    "\n",
    "            # Save processed data using Path\n",
    "            np.save(Path(self.config.root_dir) / \"train_processed.npy\", train_combined)\n",
    "            np.save(Path(self.config.root_dir) / \"test_processed.npy\", test_combined)\n",
    "\n",
    "            logger.info(\"Preprocessed train and test data saved successfully.\")\n",
    "            return train_processed, test_processed\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in preprocess_features: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-21 22:57:34,992: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-04-21 22:57:34,996: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-04-21 22:57:35,023: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-04-21 22:57:35,023: INFO: common: created directory at: artifacts]\n",
      "[2025-04-21 22:57:35,023: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-04-21 22:57:35,023: INFO: 3660953481: Loading data from artifacts/data_cleaning/cleaned_flight_data.csv]\n",
      "[2025-04-21 22:57:35,306: INFO: 3660953481: Saved label encoders to artifacts/data_transformation/label_encoders.pkl]\n",
      "[2025-04-21 22:57:35,513: INFO: 3660953481: Split data into training and test sets]\n",
      "[2025-04-21 22:57:35,522: INFO: 3660953481: Training data shape: (42750, 10)]\n",
      "[2025-04-21 22:57:35,523: INFO: 3660953481: Test data shape: (14250, 10)]\n",
      "[2025-04-21 22:57:35,527: INFO: 3660953481: Numerical columns: ['Airline', 'Source', 'Destination', 'Stopovers', 'Class', 'Booking Source', 'Days Before Departure', 'Departure Time', 'Arrival Time']]\n",
      "[2025-04-21 22:57:35,539: INFO: 3660953481: Categorical columns: []]\n",
      "[2025-04-21 22:57:35,600: INFO: 3660953481: Preprocessor saved at artifacts/data_transformation/preprocessor.pkl]\n",
      "[2025-04-21 22:57:35,606: INFO: 3660953481: Preprocessed train and test data saved successfully.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    train, test = data_transformation.train_test_splitting()\n",
    "    train_processed, test_processed = data_transformation.preprocess_features(train, test)\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"File not found: {e}\")\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Missing key in configuration: {e}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Unexpected error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flight-fare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
